{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Set-up"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from tensorforce.agents import Agent"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "np.random.seed(0)\r\n",
    "n_train = 10\r\n",
    "X_train = pd.DataFrame(np.random.uniform(size=n_train), columns=[\"A\"])\r\n",
    "Z_train =  pd.DataFrame(np.array([np.random.uniform(size=n_train), np.random.uniform(size=n_train)]).T, columns=[\"B\", \"C\"])\r\n",
    "Y_train = pd.DataFrame(np.random.uniform(size=n_train), columns=[\"D\"])\r\n",
    "\r\n",
    "agent = Agent.create(\r\n",
    "    agent='tensorforce', states=dict(type='float', shape=(2,), min_value=(0,0), max_value=(1,1)),\r\n",
    "    actions=dict(type='float', shape=(1,), min_value=0, max_value=1),\r\n",
    "    max_episode_timesteps=2, update=64,\r\n",
    "    optimizer=dict(optimizer='adam', learning_rate=1e-3),\r\n",
    "    objective='policy_gradient', reward_estimation=dict(horizon=1),\r\n",
    "    exploration=10.0, parallel_interactions=n_train\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1st Bug"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bugs in batch mode of experience"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# trying batch mode with list of dictionaries\r\n",
    "states = [{\"state\": np.array(Z_train.iloc[i])} for i in range(len(Z_train))]\r\n",
    "actions = [{\"action\": np.array(X_train.iloc[i])} for i in range(len(X_train))]\r\n",
    "rewards = np.reshape(np.array(Y_train), (-1,))\r\n",
    "terminal = np.full(rewards.shape, True)\r\n",
    "\r\n",
    "agent.experience(states=states, actions=actions, terminal=terminal, reward=rewards)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TensorforceError",
     "evalue": "Invalid value for Agent.experience actions argument value: ArrayDict(action=[[0.5488135]]).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTensorforceError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ISHAN~1.PAI\\AppData\\Local\\Temp/ipykernel_22728/1824268970.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mterminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\causal\\lib\\site-packages\\tensorforce\\agents\\tensorforce.py\u001b[0m in \u001b[0;36mexperience\u001b[1;34m(self, states, actions, terminal, reward, internals)\u001b[0m\n\u001b[0;32m    625\u001b[0m             )\n\u001b[0;32m    626\u001b[0m             actions_batch = self.actions_spec.to_tensor(\n\u001b[1;32m--> 627\u001b[1;33m                 \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Agent.experience actions'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m             )\n\u001b[0;32m    629\u001b[0m             terminal_batch = self.terminal_spec.to_tensor(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\causal\\lib\\site-packages\\tensorforce\\core\\utils\\tensors_spec.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(self, value, batched, recover_empty, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# TODO: improve exception message to include invalid keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecover_empty\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTensorforceError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTensorforceError\u001b[0m: Invalid value for Agent.experience actions argument value: ArrayDict(action=[[0.5488135]])."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trying batch mode with np.ndarray for actions\r\n",
    "states = [{\"state\": np.array(Z_train.iloc[i])} for i in range(len(Z_train))]\r\n",
    "actions = np.array(X_train)\r\n",
    "rewards = np.reshape(np.array(Y_train), (-1,))\r\n",
    "terminal = np.full(rewards.shape, True)\r\n",
    "\r\n",
    "agent.experience(states=states, actions=actions, terminal=terminal, reward=rewards)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# trying batch mode with list of arrays for actions\r\n",
    "states = [{\"state\": np.array(Z_train.iloc[i])} for i in range(len(Z_train))]\r\n",
    "actions = list(np.array(X_train))\r\n",
    "rewards = np.reshape(np.array(Y_train), (-1,))\r\n",
    "terminal = np.full(rewards.shape, True)\r\n",
    "\r\n",
    "agent.experience(states=states, actions=actions, terminal=terminal, reward=rewards)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working code for experience"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# loop through training set\r\n",
    "for i in range(len(X_train)):\r\n",
    "    # define current state, action, reward in correct format\r\n",
    "    current_state = [np.array(Z_train.iloc[i])]\r\n",
    "    current_action = [np.array(X_train.iloc[i])]\r\n",
    "    current_reward = list(Y_train.iloc[i])\r\n",
    "\r\n",
    "    agent.experience(states=current_state, actions=current_action, terminal=[True], reward=current_reward)\r\n",
    "    agent.update()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2nd Bug\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bugs in input format for experience"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# trying experience for 1 episode using dictionary for actions\r\n",
    "states = {\"state\": np.array(Z_train.iloc[0])}\r\n",
    "actions = {\"action\": np.array(X_train.iloc[0])}\r\n",
    "rewards = np.reshape(np.array(Y_train.iloc[0]), ())\r\n",
    "terminal = np.full(rewards.shape, True)\r\n",
    "\r\n",
    "agent.experience(states=states, actions=actions, terminal=terminal, reward=rewards)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TensorforceError",
     "evalue": "Invalid value for Agent.experience actions argument value: ArrayDict(SINGLETON=ArrayDict(action=[[0.5488135]])).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTensorforceError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ISHAN~1.PAI\\AppData\\Local\\Temp/ipykernel_22728/1152040306.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mterminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\causal\\lib\\site-packages\\tensorforce\\agents\\tensorforce.py\u001b[0m in \u001b[0;36mexperience\u001b[1;34m(self, states, actions, terminal, reward, internals)\u001b[0m\n\u001b[0;32m    625\u001b[0m             )\n\u001b[0;32m    626\u001b[0m             actions_batch = self.actions_spec.to_tensor(\n\u001b[1;32m--> 627\u001b[1;33m                 \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Agent.experience actions'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m             )\n\u001b[0;32m    629\u001b[0m             terminal_batch = self.terminal_spec.to_tensor(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\causal\\lib\\site-packages\\tensorforce\\core\\utils\\tensors_spec.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(self, value, batched, recover_empty, name)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# TODO: improve exception message to include invalid keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecover_empty\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTensorforceError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'value'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTensorDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTensorforceError\u001b[0m: Invalid value for Agent.experience actions argument value: ArrayDict(SINGLETON=ArrayDict(action=[[0.5488135]]))."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# trying experience for 1 episode using np.ndarray for actions\r\n",
    "states = {\"state\": np.array(Z_train.iloc[0])}\r\n",
    "actions = np.array(X_train.iloc[0])\r\n",
    "rewards = np.reshape(np.array(Y_train.iloc[0]), ())\r\n",
    "terminal = np.full(rewards.shape, True)\r\n",
    "\r\n",
    "agent.experience(states=states, actions=actions, terminal=terminal, reward=rewards)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TensorforceError",
     "evalue": "Invalid type for Agent.experience argument actions: <class 'numpy.ndarray'> is not dict.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTensorforceError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ISHAN~1.PAI\\AppData\\Local\\Temp/ipykernel_22728/1891561514.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mterminal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\causal\\lib\\site-packages\\tensorforce\\agents\\tensorforce.py\u001b[0m in \u001b[0;36mexperience\u001b[1;34m(self, states, actions, terminal, reward, internals)\u001b[0m\n\u001b[0;32m    534\u001b[0m                 raise TensorforceError.type(\n\u001b[0;32m    535\u001b[0m                     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Agent.experience'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margument\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'actions'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 536\u001b[1;33m                     \u001b[0mhint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'is not dict'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    537\u001b[0m                 )\n\u001b[0;32m    538\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTensorforceError\u001b[0m: Invalid type for Agent.experience argument actions: <class 'numpy.ndarray'> is not dict."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Working code for experience"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# trying experience for 1 episode using list for actions\r\n",
    "states = {\"state\": np.array(Z_train.iloc[0])}\r\n",
    "actions = list(np.array(X_train.iloc[0]))\r\n",
    "rewards = np.reshape(np.array(Y_train.iloc[0]), ())\r\n",
    "terminal = np.full(rewards.shape, True)\r\n",
    "\r\n",
    "agent.experience(states=states, actions=actions, terminal=terminal, reward=rewards)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('causal': conda)"
  },
  "interpreter": {
   "hash": "474060183c04b18c5f1e4efae9a7835293caaa3f3d64a6a241eb0ef0118553d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}